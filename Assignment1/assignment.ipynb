{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "import os\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Crawler:\n",
    "    def __init__(self, baseUrl: str = None, keywords: list = None):\n",
    "        \"\"\"\n",
    "        Constructor to initialize crawler and attributes of the crawler like baseUrl, header and keywords.\n",
    "        The constructor initializes a queue for BFS crawling from the websites and output directory for the crawled data.\n",
    "        \"\"\"\n",
    "        self.baseUrl = baseUrl\n",
    "        self.header = {\n",
    "            \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8\",\n",
    "            \"accept-encoding\": \"gzip, deflate, br\",\n",
    "            \"accept-language\": \"en-US,en;q=0.9\",\n",
    "            \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36\",\n",
    "            \"connection\": \"keep-alive\",\n",
    "            \"cache-control\": \"no-cache\",\n",
    "        }\n",
    "        # words to avoid in the url so as to be not directed to pages like login and signup\n",
    "        self._exclude = [\n",
    "            \"section\", \"for-authors\",\n",
    "            \"editorial-board\", \"about\", \"issues\",\n",
    "            \"posts\", \"toggle\", \"view=desktop\", \"search\", \"signup\", \"login\"\n",
    "        ]\n",
    "        # keywords to search in the crawled document to avoid bad pages\n",
    "        self._keywords = keywords or [\n",
    "            \"healthcare\", \"health\", \"hospital\", \"clinic\", \"emergency\", \"primary care\",\n",
    "            \"disease\", \"infection\", \"symptoms\", \"diagnosis\", \"treatment\", \"therapy\",\n",
    "            \"chronic\", \"acute\", \"mental health\", \"cancer\", \"diabetes\", \"cardiology\",\n",
    "            \"clinical trial\", \"meta-analysis\", \"systematic review\", \"case study\",\n",
    "            \"epidemiology\", \"randomized controlled trial\", \"biostatistics\",\n",
    "            \"nutrition\", \"exercise\", \"stress\", \"prevention\", \"self-care\", \"fitness\",\n",
    "            \"vaccination\", \"immunization\", \"screening\",\n",
    "            \"health policy\", \"public health\", \"health reform\", \"universal healthcare\",\n",
    "            \"insurance\", \"health equity\", \"pandemic\", \"quarantine\", \"outbreak\"\n",
    "        ]\n",
    "        self._visited = set()  # Tracks visited URLs\n",
    "        self.queue = []  # Queue for URLs to crawl\n",
    "        self._data_loc = \"./data/\"  # Directory to save output data\n",
    "        os.makedirs(self._data_loc, exist_ok=True)\n",
    "\n",
    "    def _crawl(self, currUrl: str = None):\n",
    "        \"\"\"\n",
    "        Helper function to crawl a single website.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            page = requests.get(currUrl, headers=self.header)\n",
    "            page.raise_for_status()  \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching {currUrl}: {e}\")\n",
    "            return None\n",
    "        soup = BeautifulSoup(page.text, 'html.parser')\n",
    "\n",
    "        # Add links from the current page to the queue for further crawling\n",
    "        for link in soup.find_all('a', href=True):\n",
    "            fullUrl = urljoin(currUrl, link['href'])\n",
    "            if fullUrl not in self._visited and fullUrl.startswith(\"http\") and not any(keyword in fullUrl for keyword in self._exclude):\n",
    "                self._visited.add(fullUrl)\n",
    "                self.queue.append(fullUrl)\n",
    "        return soup\n",
    "\n",
    "    def crawl_n(self, n: int, output_file: str):\n",
    "        \"\"\"\n",
    "        Crawls 'n' documents and save all data to a single CSV file.\n",
    "        \"\"\"\n",
    "        count = 0\n",
    "        data = []  # Store all crawled data\n",
    "\n",
    "        while count < n and self.queue:\n",
    "            currUrl = self.queue.pop(0)\n",
    "            print(f\"Crawling: {currUrl}\")\n",
    "\n",
    "            soup = self._crawl(currUrl)\n",
    "            if not soup:\n",
    "                continue\n",
    "\n",
    "            # Extract the title of the webpage\n",
    "            title = soup.title.string.strip() if soup.title else \"No Title\"\n",
    "\n",
    "            # Extract and clean the page content\n",
    "            content = soup.get_text(separator=' ')\n",
    "            content = content.replace(\"\\n\", \" \").replace(\"\\r\", \" \").strip()\n",
    "\n",
    "            # Check for relevant keywords in the content\n",
    "            if any(keyword in content.lower() for keyword in self._keywords):\n",
    "                data.append({\n",
    "                    \"URL\": currUrl,\n",
    "                    \"Title\": title,\n",
    "                    \"Content\": content\n",
    "                })\n",
    "                print(f\"Document {count + 1} crawled: {title}\")\n",
    "                count += 1\n",
    "\n",
    "        # Save all crawled data to a single CSV file\n",
    "        with open(output_file, mode='w', encoding='utf-8', newline='') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=[\"URL\", \"Title\", \"Content\"])\n",
    "            writer.writeheader()\n",
    "            writer.writerows(data)\n",
    "\n",
    "        print(f\"All {count} documents saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List of starting URLs for the crawler\n",
    "starting_urls = [\n",
    "    \"https://healthcare-digital.com/top10/top-10-healthcare-websites\",\n",
    "    # \"https://pubmed.ncbi.nlm.nih.gov/\",\n",
    "    \"https://www.cdc.gov/\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Crawling: https://healthcare-digital.com/top10/top-10-healthcare-websites\n",
      "Document 1 crawled: Top 10 Healthcare Websites | Healthcare Digital\n",
      "Crawling: https://www.cdc.gov/\n",
      "Document 2 crawled: Centers for Disease Control and Prevention | CDC\n",
      "Crawling: https://healthcare-digital.com/\n",
      "Document 3 crawled: Home of Healthcare News | Healthcare Digital\n",
      "Crawling: http://www.nih.gov\n",
      "Document 4 crawled: National Institutes of Health (NIH) | Turning Discovery Into Health\n",
      "Crawling: http://kidshealth.org\n",
      "Document 5 crawled: Nemours KidsHealth - the Web's most visited site about children's health\n",
      "Crawling: http://www.webmd.com\n",
      "Document 6 crawled: WebMD - Better information. Better health.\n",
      "Crawling: http://www.boots.webmd.com\n",
      "Error fetching http://www.boots.webmd.com: HTTPSConnectionPool(host='www.webmd.boots.com', port=443): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x0000019B16FB2750>: Failed to resolve 'www.webmd.boots.com' ([Errno 11002] getaddrinfo failed)\"))\n",
      "Crawling: https://www.drugs.com\n",
      "Error fetching https://www.drugs.com: 403 Client Error: Forbidden for url: https://www.drugs.com/\n",
      "Crawling: http://health.yahoo.net\n",
      "Document 7 crawled: Yahoo Life: Latest News on Health, Wellness, Style, Fashion Trends and More\n",
      "Crawling: http://www.weightwatchers.com\n",
      "Document 8 crawled: Weight-Loss Program: Lose Weight. Gain Health | WeightWatchers\n",
      "Crawling: http://www.nhsdirect.nhs.uk\n",
      "Error fetching http://www.nhsdirect.nhs.uk: HTTPConnectionPool(host='www.nhsdirect.nhs.uk', port=80): Max retries exceeded with url: / (Caused by NameResolutionError(\"<urllib3.connection.HTTPConnection object at 0x0000019B198FDDF0>: Failed to resolve 'www.nhsdirect.nhs.uk' ([Errno 11001] getaddrinfo failed)\"))\n",
      "Crawling: http://www.netdoctor.co.uk\n",
      "Document 9 crawled: Health and longevity including mental health, conditions and gynae issues - Women's Health UK\n",
      "Crawling: http://www.mayoclinic.com\n",
      "Document 10 crawled: Top-ranked Hospital in the Nation - Mayo Clinic\n",
      "Crawling: http://www.menshealth.com\n",
      "Document 11 crawled: Men's Health - Fitness, Nutrition, Health, Sex, Style & Weight Loss Tips for Men\n",
      "Crawling: http://itunes.apple.com/us/app/healthcare-global/id450637397?mt=8\n",
      "Error fetching http://itunes.apple.com/us/app/healthcare-global/id450637397?mt=8: 404 Client Error: Not Found for url: https://apps.apple.com/us/app/healthcare-global/id450637397\n",
      "Crawling: https://healthcare-digital.com/magazine/healthcare-digital-september-2024\n",
      "Document 12 crawled: Healthcare Digital - September 2024 | Healthcare Digital\n",
      "Crawling: https://healthcare-digital.com/top10/top-10-consumer-healthcare-companies\n",
      "Document 13 crawled: Top 10: Consumer Healthcare Companies | Healthcare Digital\n",
      "Crawling: https://healthcare-digital.com/top10/top-10-healthcare-technology-companies\n",
      "Document 14 crawled: Top 10: Healthcare Technology Companies | Healthcare Digital\n",
      "Crawling: https://healthcare-digital.com/hospitals/worlds-8-most-architecturally-beautiful-hospitals\n",
      "Document 15 crawled: The World's 8 Most Architecturally Beautiful Hospitals | Healthcare Digital\n",
      "Crawling: https://healthcare-digital.com/technology-and-ai/top-10-mhealth-apps-android-and-ios-2015\n",
      "Document 16 crawled: TOP 10: mHealth Apps for Android and iOS of 2015 | Healthcare Digital\n",
      "Crawling: https://healthcare-digital.com/hospitals/top-10-chief-human-resources-officers-chros-in-healthcare\n",
      "Error fetching https://healthcare-digital.com/hospitals/top-10-chief-human-resources-officers-chros-in-healthcare: HTTPSConnectionPool(host='healthcare-digital.com', port=443): Max retries exceeded with url: /hospitals/top-10-chief-human-resources-officers-chros-in-healthcare (Caused by ConnectTimeoutError(<urllib3.connection.HTTPSConnection object at 0x0000019B18909370>, 'Connection to healthcare-digital.com timed out. (connect timeout=None)'))\n",
      "Crawling: https://healthcare-digital.com/technology-and-ai/top-10-blockchain-platforms-in-healthcare\n",
      "Document 17 crawled: Top 10 Healthcare Blockchain Platforms Revolutionizing Medical Data Management | Healthcare Digital\n",
      "Crawling: https://healthcare-digital.com/digital-healthcare/top-10-digital-diagnostics-platforms\n"
     ]
    }
   ],
   "source": [
    "# Initialize the crawler\n",
    "crawler = Crawler()\n",
    "crawler.queue.extend(starting_urls)  # Add starting URLs to the queue\n",
    "crawler.crawl_n(300, output_file=\"./data/combined_crawled_data.csv\") # Crawl 500 documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextPreprocessor:\n",
    "    def __init__(self, input_dir=\"./data/\", output_dir=\"./processed_data/\"):\n",
    "        self.input_dir = input_dir\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Remove HTML tags, special characters, numbers, and extra spaces.\"\"\"\n",
    "        text = re.sub(r'<[^>]*>', '', text)  # Remove HTML tags\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)  # Remove special characters\n",
    "        text = re.sub(r'\\d+', '', text)      # Remove numbers\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "        return text\n",
    "\n",
    "    def remove_stopwords(self, words):\n",
    "        \"\"\"Remove stopwords from a list of words.\"\"\"\n",
    "        return [word for word in words if word.lower() not in self.stop_words]\n",
    "\n",
    "    def stem_words(self, words):\n",
    "        \"\"\"Apply stemming to a list of words.\"\"\"\n",
    "        return [self.stemmer.stem(word) for word in words]\n",
    "\n",
    "    def lemmatize_words(self, words):\n",
    "        \"\"\"Apply lemmatization to a list of words.\"\"\"\n",
    "        return [self.lemmatizer.lemmatize(word) for word in words]\n",
    "\n",
    "    def process_file(self, input_file, output_file):\n",
    "        \"\"\"Process a single file: clean, tokenize, and apply stemming/lemmatization.\"\"\"\n",
    "        with open(input_file, 'r', encoding='utf-8') as file:\n",
    "            reader = csv.DictReader(file)\n",
    "            processed_data = []\n",
    "\n",
    "            for row in reader:\n",
    "                content = row['Content']\n",
    "                # Clean the text\n",
    "                cleaned_text = self.clean_text(content)\n",
    "\n",
    "                # Tokenize into sentences and words\n",
    "                sentences = sent_tokenize(cleaned_text)\n",
    "                word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "                # Remove stopwords and apply stemming/lemmatization\n",
    "                processed_sentences = []\n",
    "                for tokens in word_tokens:\n",
    "                    filtered_tokens = self.remove_stopwords(tokens)\n",
    "                    stemmed_tokens = self.stem_words(filtered_tokens)  # You can switch to lemmatization\n",
    "                    processed_sentences.append(\" \".join(stemmed_tokens))\n",
    "\n",
    "                # Join processed sentences\n",
    "                processed_content = \" \".join(processed_sentences)\n",
    "                processed_data.append({\n",
    "                    \"URL\": row['URL'],\n",
    "                    \"Title\": row['Title'],\n",
    "                    \"Processed_Content\": processed_content\n",
    "                })\n",
    "\n",
    "        # Save the processed data to a new CSV file\n",
    "        with open(output_file, 'w', encoding='utf-8', newline='') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=[\"URL\", \"Title\", \"Processed_Content\"])\n",
    "            writer.writeheader()\n",
    "            writer.writerows(processed_data)\n",
    "    def process_all_files(self):\n",
    "        \"\"\"Process all CSV files in the input directory and save a combined CSV.\"\"\"\n",
    "        combined_data = []  # List to store all rows across files\n",
    "\n",
    "        for file_name in os.listdir(self.input_dir):\n",
    "            if file_name.endswith(\".csv\"):\n",
    "                input_file = os.path.join(self.input_dir, file_name)\n",
    "                print(f\"Processing {input_file}...\")\n",
    "\n",
    "                # Read and process each file\n",
    "                with open(input_file, 'r', encoding='utf-8') as file:\n",
    "                    reader = csv.DictReader(file)\n",
    "\n",
    "                    for row in reader:\n",
    "                        content = row['Content']\n",
    "\n",
    "                        # Clean the text\n",
    "                        cleaned_text = self.clean_text(content)\n",
    "\n",
    "                        # Tokenize into sentences and words\n",
    "                        sentences = sent_tokenize(cleaned_text)\n",
    "                        word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "\n",
    "                        # Remove stopwords and apply stemming/lemmatization\n",
    "                        processed_sentences = []\n",
    "                        for tokens in word_tokens:\n",
    "                            filtered_tokens = self.remove_stopwords(tokens)\n",
    "                            stemmed_tokens = self.stem_words(filtered_tokens)  # You can switch to lemmatization\n",
    "                            processed_sentences.append(\" \".join(stemmed_tokens))\n",
    "\n",
    "                        # Join processed sentences\n",
    "                        processed_content = \" \".join(processed_sentences)\n",
    "                        combined_data.append({\n",
    "                            \"URL\": row['URL'],\n",
    "                            \"Title\": row['Title'],\n",
    "                            \"Processed_Content\": processed_content\n",
    "                        })\n",
    "        # Save the combined data to a single CSV file\n",
    "        combined_file_path = os.path.join(self.output_dir, 'combined_processed_data.csv')\n",
    "        with open(combined_file_path, 'w', encoding='utf-8', newline='') as file:\n",
    "            writer = csv.DictWriter(file, fieldnames=[\"URL\", \"Title\", \"Processed_Content\"])\n",
    "            writer.writeheader()\n",
    "            writer.writerows(combined_data)\n",
    "\n",
    "        print(f\"All processed data saved to {combined_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import sys\n",
    "\n",
    "# Increase the CSV field size limit\n",
    "# csv.field_size_limit(sys.maxsize)\n",
    "csv.field_size_limit(2**31 - 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and run the text preprocessor\n",
    "preprocessor = TextPreprocessor()\n",
    "preprocessor.process_all_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "def word_frequency_analysis():\n",
    "    input_csv= './processed_data/combined_processed_data.csv'\n",
    "    output_csv=  './output/word_frequency.csv'\n",
    "    all_text = []\n",
    "    \n",
    "    sentence_length=[]\n",
    "    with open(input_csv, 'r', encoding='utf-8') as file:\n",
    "        reader = csv.DictReader(file)\n",
    "        for row in reader:\n",
    "            processed_content = row['Processed_Content']\n",
    "            all_text.append(processed_content)\n",
    "            \n",
    "            sentences = processed_content.split(\".\")\n",
    "            sentence_length.extend([len(sentence.split()) for sentence in sentences if sentence.strip()])\n",
    "    \n",
    "    combined_text = \" \".join(all_text)\n",
    "    words = combined_text.split()\n",
    "    word_counts = Counter(words)\n",
    "    most_common_words = word_counts.most_common(20)  # Top 20 words\n",
    "    \n",
    "    word_freq_df = pd.DataFrame(most_common_words, columns=['Word', 'Frequency'])\n",
    "    \n",
    "    output_dir = os.path.dirname(output_csv)\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    with open(output_csv, 'w', encoding='utf-8', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Word', 'Frequency'])\n",
    "        writer.writerows(word_counts.most_common())\n",
    "              \n",
    "    \n",
    "    print(f\"Word frequency analysis saved to {output_csv}\")\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x='Frequency', y='Word', data=word_freq_df, palette='deep')\n",
    "    plt.title('Top 20 Most Frequent Words', fontsize=16)\n",
    "    plt.xlabel('Frequency', fontsize=12)\n",
    "    plt.ylabel('Word', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./output/top20wordfrequency.png')\n",
    "    plt.show()\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.histplot(sentence_length, kde=True, bins=30, color='blue')\n",
    "    plt.title('Distribution of Sentence Lengths', fontsize=16)\n",
    "    plt.xlabel('Sentence Length (Number of Words)', fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('./output/sentence_length_distribution.png')  # Save the chart\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_frequency_analysis()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
